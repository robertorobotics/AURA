export interface BlogPost {
  slug: string;
  title: string;
  date: string;
  summary: string;
  tags: string[];
  content: string; // Plain text with \n\n paragraph breaks
}

export const BLOG_POSTS: BlogPost[] = [
  {
    slug: "assembly-graph",
    title: "How AURA's Assembly Graph Works",
    date: "2025-02-01",
    tags: ["architecture", "assembly"],
    summary:
      "The assembly graph is the central data model in AURA. Every module — execution, recording, training, analytics — is indexed by step_id.",
    content: `The assembly graph is the spine of AURA. Every module in the system — execution, recording, training, analytics — references the same data structure, and everything is indexed by step_id. At its core, an AssemblyGraph is a Pydantic model with three fields: a parts dictionary keyed by part ID (each Part carrying its geometry classification, mesh file path, assembled position, rotation, color, and grasp points), a steps dictionary keyed by step ID, and a step_order list that defines the topologically sorted execution sequence. This design means that when a recorder captures a demonstration, when a trainer produces a policy checkpoint, when the analytics store logs a run metric, or when the execution sequencer retries a failed step, they all reference the exact same step_id. There is no translation layer, no mapping table — one identifier threads through the entire stack.

Each AssemblyStep carries everything needed to execute it. The id and name are human-readable (e.g., "step_012", "Insert bearing into housing"). The part_ids list identifies which parts are involved. The dependencies list names the step IDs that must complete before this step can begin — this is what makes the graph a DAG rather than a flat list. The handler field selects the execution strategy: "primitive" for hard-coded motion commands (pick, place, press_fit, linear_insert, screw, guarded_move, move_to), "policy" for a learned behavior cloning checkpoint, or "rl_finetune" for a policy refined through online reinforcement learning. Each step also carries success_criteria with a typed verification condition — position tolerance, force_threshold in Newtons, a trained classifier, or a force_signature pattern like snap_fit or meshing — that the system checks after execution to decide whether the step actually succeeded. The max_retries field (default 3) controls how many attempts the sequencer makes before escalating to a human.

The graph is generated automatically from a STEP file. The CAD parser first attempts an XDE (Extended Data Exchange) tree traversal, which preserves the assembly hierarchy by walking the TDF_Label tree and composing parent-child transforms at each level so that leaf shapes end up with their correct global position. If the XDE reader yields zero parts — which happens with some STEP exporters — it falls back to a flat reader that extracts all solids via STEPControl_Reader and uses their centroids as positions. After extraction, the parser detects units by checking bounding box extents: if the maximum coordinate exceeds 1.0, the file is assumed to be in millimeters and a 0.001 scale factor is applied. Each part is tessellated into a GLB mesh for the 3D viewer, its geometry is classified as box, cylinder, or sphere based on dimension ratios, and pairwise contact detection runs — an O(n-squared) BRepExtrema_DistShapeShape distance check that flags any pair of parts closer than 0.2mm as being in contact. Contact detection operates on the raw unscaled shapes for accuracy, while tessellation and geometry classification use the scaled values.

The sequence planner turns this parsed assembly into executable steps. It starts by separating covers (thin, wide parts) from interior parts, then sorting: the largest non-cover part by volume becomes the base (it stays fixed on the worksurface), interior parts are ordered by Y-position ascending then volume descending (bottom-up assembly), and covers go last. For each non-base part, the planner generates a pick step and an assembly step, then classifies the assembly action based on geometry and contacts. Small cylinders with contacts (radius under 8mm) get assigned to a learned policy — they are too fiddly for open-loop primitives. Fastener-sized parts (volume under 1e-6 cubic meters) become press_fit primitives with a 15N force threshold. Parts with three or more contacts, or those matching keywords like gear, bearing, or snap, also default to policy with a classifier-based success check. The planner targets about 70% accuracy; the operator adjusts the rest through the frontend. Finally, Kahn's algorithm topologically sorts the steps by their dependency edges, producing the step_order list.

The execution sequencer walks step_order as a state machine. It starts in IDLE, transitions to RUNNING when the operator hits start, then enters a loop: for each step in step_order, it marks the step as STEP_ACTIVE, dispatches it through the policy router (which selects the right primitive function, BC policy, or RL policy based on the handler field), and checks the result. If the step succeeds and an optional StepVerifier confirms it (converting force history to scalar magnitudes and checking against success_criteria), the step is marked STEP_COMPLETE and the sequencer advances. If it fails, the sequencer retries up to max_retries times with a 0.5-second backoff between attempts. If all retries are exhausted, it transitions to WAITING_FOR_HUMAN — the operator steps in via teleop and signals completion. Every attempt, successful or not, is recorded in the analytics store with the assembly_id, step_id, duration, and attempt count. The entire execution state — phase, current step, per-step statuses, run number, elapsed time, overall success rate — is broadcast over WebSocket so the frontend dashboard updates in real time.`,
  },
  {
    slug: "force-feedback-teleoperation",
    title: "Force Feedback Teleoperation for Assembly Teaching",
    date: "2025-01-20",
    tags: ["control", "hardware"],
    summary:
      "Teaching a robot to assemble by feel. Our leader-follower system lets operators feel the forces during assembly.",
    content: `AURA uses a leader-follower teleoperation architecture where the human operates lightweight Dynamixel XL330 leader arms while heavier Damiao actuators do the actual assembly work. The split is deliberate: Dynamixel servos are cheap, backdrivable, and comfortable for a human to manipulate — they are input devices. The Damiao J8009P (shoulder, 35Nm rated), J4340P (elbow, 8Nm rated), and J4310 (wrist, 4Nm rated) motors provide the torque needed for press-fits, gear meshing, and snap-fit insertion. Each follower arm has 7 degrees of freedom running MIT-mode impedance control over CAN bus (kp=30, kd=1.5 on shoulder and elbow, kp=15, kd=0.25 on wrist). The leader and follower are paired through a joint mapping module that translates Dynamixel joint names to Damiao joint names and converts between their different value representations — radians on one side, percentage-of-range on the other.

The control loop runs at 60Hz, giving a 16.67ms frame budget. Each frame follows a strict pipeline: read the leader arm positions (with up to 3 retries on transient serial errors like "Incorrect status packet" with 5ms backoff), apply leader assist (gravity compensation from a fitted sinusoidal model on sin/cos joint features, plus friction assist via tanh-shaped negative damping with a 1.0 deg/s velocity deadband), map the joint values from Dynamixel space to Damiao space, apply a 2-second startup blend that linearly interpolates from the follower's actual position to the leader's target, send the action to the follower, run safety checks at 10Hz (every 6th frame to stay within the frame budget), and finally apply force feedback back to the leader. The startup blend is critical — without it, any mismatch between where the leader and follower start would cause the follower to snap to the leader's position at full speed. Performance is logged every 60 frames (once per second) as actual Hz.

Force feedback is what makes assembly teaching practical rather than blind. The gripper channel maps follower gripper torque to a leader current ceiling: the raw torque is smoothed with an exponential moving average (alpha=0.3, giving roughly 55ms of smoothing at 60Hz), then a dead zone below 0.2Nm filters out noise. Between 0.2Nm and the 2.0Nm saturation point, the torque maps linearly to a goal current between 60mA (baseline idle stiffness) and 1750mA (full saturation). The operator feels this as the leader gripper becoming progressively harder to squeeze as the follower grips tighter. For the arm joints, a virtual spring model applies: the position error between leader and follower is computed, and if it exceeds a 0.1-radian deadzone, a spring constant of 15,000 mA/rad pulls the leader toward the follower's actual position with a minimum force of 100mA. Within the deadzone, the leader is completely limp — the human moves freely. Outside it, resistance increases sharply, telling the operator that the follower is lagging or hitting something.

Force feedback transforms assembly teaching from a frustrating guessing game into an intuitive physical dialogue. When a bearing snaps into its housing, the operator feels the characteristic force spike through the gripper — a sharp resistance followed by a sudden give. Gear meshing produces rhythmic resistance as teeth engage. Press-fits build steady force that the operator can dose precisely. Without this feedback, operators routinely over-apply force (risking damage to parts or the robot) or under-apply it (failing to seat components). In our testing, force feedback reduces the number of demonstrations needed for a reliable policy by roughly an order of magnitude. Operators learn the feel of a successful assembly within 2-3 attempts and can consistently reproduce it, giving the behavior cloning pipeline clean, successful demonstrations rather than noisy failed ones that dilute the dataset.

The safety layer runs alongside the control loop with conservative torque limits set at 10% of each motor's rated maximum: 3.5Nm for the J8009P, 0.8Nm for the J4340P, and 0.4Nm for the J4310. Violations are debounced: a motor must exceed its torque limit on 3 consecutive checks before triggering an emergency stop, preventing false positives from transient force spikes during normal assembly contact. The CAN bus is monitored every frame — if the bus goes dead (cable disconnect, driver crash), the safety layer triggers immediately. An emergency stop calls robot.disconnect(), cutting power to all follower arms, and raises a SafetyError that halts the sequencer. Feetech-based setups use a round-robin checking scheme (one motor per frame) to avoid saturating the serial bus. The entire safety and force feedback pipeline completes well within the 16.67ms frame budget, keeping the control loop responsive enough that the round-trip latency from contact to haptic sensation stays under one frame.`,
  },
  {
    slug: "per-step-learning",
    title: "From Demos to Policies: Per-Step Learning in AURA",
    date: "2025-01-10",
    tags: ["learning", "ML"],
    summary:
      "Each assembly step gets its own policy, trained from a handful of demonstrations and fine-tuned with human corrections.",
    content: `The recording pipeline is the entry point for all learning in AURA. When the operator selects an assembly step and starts a recording session, a DemoRecorder spins up a daemon thread that captures observations at 50Hz — a separate thread from the 60Hz control loop to avoid blocking motor commands with file I/O. Each frame captures joint positions, gripper state, force/torque readings from the follower, and the action (joint positions) being sent. The recorder grabs the latest action through a thread-safe lock on the teleop loop's cached action buffer. When the operator finishes and stops recording, the buffered frames are flushed to an HDF5 file at data/demos/{assembly_id}/{step_id}/{demo_id}.hdf5, with metadata attributes (assembly_id, step_id, timestamp, frame count, duration) and datasets organized under observation/ (joint_positions, gripper_state, force_torque) and action/ (joint_positions) groups. The HDF5 format was chosen for its efficiency with large numerical arrays and compatibility with LeRobot's data loading pipeline.

Behavior cloning turns these demonstrations into executable policies. The HDF5 demos feed into LeRobot's training pipeline, which supports ACT (Action Chunking with Transformers) and Diffusion Policy architectures. The key design decision is per-step training: each assembly step gets its own policy checkpoint, stored at data/policies/{assembly_id}/{step_id}/policy.pt. A "pick gear_shaft" step has a different policy from "insert gear_shaft into housing." This means you can retrain the policy for one difficult step — say, a tight press-fit that keeps failing — without touching the policies for the 56 other steps in a gearbox assembly. The policy router in the execution layer checks the step's handler field and loads the corresponding checkpoint at inference time. If the handler is "primitive," it dispatches to a hard-coded motion command. If "policy," it loads the BC checkpoint. If "rl_finetune," it prefers the RL-refined version (policy_rl.pt) when it exists.

For steps where behavior cloning alone is not reliable enough, AURA applies HIL-SERL (Human-in-the-Loop Sample-Efficient Reinforcement Learning) fine-tuning. The RL agent is a Soft Actor-Critic implementation: a Gaussian actor network mapping 7 observation dimensions through two 256-unit hidden layers to action mean and log standard deviation (clamped between -20 and +2) with tanh squashing, twin Q-networks for value estimation with soft Polyak averaging (tau=0.005), and a learnable temperature parameter alpha that automatically balances exploration against exploitation. The SAC agent is initialized from the BC policy by copying the observation projection weights into the actor's first layer — this warm start means the RL agent begins with a reasonable policy rather than random exploration. The RLPD replay buffer (50,000 transition capacity, circular overwrite) is preloaded with all existing HDF5 demonstrations for that step, each transition marked as an expert intervention. The mixed sampling strategy guarantees that at least 25% of every training batch comes from expert data, preventing the agent from forgetting what the demonstrations taught it.

The online fine-tuning loop runs directly on the robot at 50Hz control frequency. Each episode executes the current step for up to 200 timesteps: the SAC agent selects actions, but an intervention detector monitors the leader arm's velocity — if it exceeds the 0.05 threshold (the operator is actively correcting), the system switches to the leader's action and tags the transition accordingly. A movement scale safety limiter caps each action delta at 50% of the commanded change, preventing dangerously large motions during early exploration. The dense reward function shapes behavior toward success: a position term of -0.1 times the L2 distance to the target pose encourages approach, a force term of +0.1 times the ratio of peak force to threshold encourages appropriate force application, and a smoothness penalty of -0.01 times the maximum action delta discourages jerky motion. At episode end, the StepVerifier checks whether the step succeeded — producing a terminal reward of +10.0 times the verifier's confidence if it passed, or -1.0 times confidence if it failed. The trainer runs up to 50 episodes with early stopping once the rolling success rate over the last 10 episodes reaches 80%. Checkpoints are saved every 10 episodes to data/policies/{assembly_id}/{step_id}/policy_rl.pt.

The per-step architecture is what makes this practical for real assembly. End-to-end policies that learn an entire assembly sequence need thousands of demonstrations and struggle with credit assignment — if step 34 fails, which of the previous 33 steps contributed to the failure? Per-step learning sidesteps this entirely. Each step has clear, verifiable success criteria: a position tolerance, a force threshold, or a classifier output. Five to ten demonstrations per step are typically sufficient for a reliable BC policy, and RL fine-tuning needs only 20-30 episodes to push success rates above 80%. The modularity is powerful: when a new part revision changes the tolerance on one press-fit, you retrain that single step's policy from a few new demos while every other step's policy remains untouched. Debugging is straightforward — the analytics store tracks per-step success rates, attempt counts, and durations across runs, so you can immediately identify which step is the bottleneck and focus your teaching effort there rather than retraining the whole system. This is the fastest path to universal assembly: simple primitives for easy steps, learned policies for hard ones, and targeted human corrections where the robot struggles.`,
  },
];

export function getPostBySlug(slug: string): BlogPost | undefined {
  return BLOG_POSTS.find((post) => post.slug === slug);
}
